<!DOCTYPE html>
<html>
<head>
    <title>Live2D Face Tracking with Web Camera</title>
</head>
<body>
    <video id="webcam" autoplay playsinline></video>
    <canvas id="live2d-canvas" width="800" height="600"></canvas>
    <script src="https://cdn.jsdelivr.net/npm/@vladmandic/face-api@4.0.3"></script>
    <script src="https://cubism.live2d.com/sdk-web/cubismcore/live2dcubismcore.min.js"></script>
    <script>
        // ページが読み込まれたら初期化を開始
        window.addEventListener('DOMContentLoaded', () => {
            initCamera();
        });

        async function initCamera() {
            // ビデオ要素とCanvas要素を取得
            const video = document.getElementById('webcam');
            const canvas = document.getElementById('live2d-canvas');
            const ctx = canvas.getContext('2d');

            // ブラウザのWebカメラにアクセス
            const stream = await navigator.mediaDevices.getUserMedia({ video: true });
            video.srcObject = stream;

            // Face-API.jsを初期化
            const faceapi = await window.faceapi.nets.tinyFaceDetector.loadFromUri('/models');
            await window.faceapi.nets.faceLandmark68Net.loadFromUri('/models');
            await window.faceapi.nets.faceRecognitionNet.loadFromUri('/models');

            const options = new window.faceapi.TinyFaceDetectorOptions({ inputSize: 512 });
            const faceTracker = new window.faceapi.FaceTracker(options);

            // Live2Dの初期化
            Live2D.init();

            // モデルの読み込み
            const modelDef = '/しーくま(Live2D用).moc3'; // モデルファイルのパス
            const model = Live2D.Model.load(modelDef, () => {
                // モデル読み込み後の処理
                model.addTo(canvas);
            });

            // カメラストリームから顔をトラッキング
            video.addEventListener('play', () => {
                const displaySize = { width: video.width, height: video.height };
                window.faceapi.matchDimensions(canvas, displaySize);

                setInterval(async () => {
                    const detections = await window.faceapi.detectAllFaces(video, options).withFaceLandmarks().withFaceDescriptors();
                    const resizedDetections = window.faceapi.resizeResults(detections, displaySize);
                    ctx.clearRect(0, 0, canvas.width, canvas.height);
                    window.faceapi.draw.drawDetections(canvas, resizedDetections);
                }, 100);
            });
        }
    </script>
</body>
</html>